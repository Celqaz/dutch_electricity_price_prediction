{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef8b833-997d-4b67-b374-d1046343b4d3",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5a003ce-984b-4ff7-9ba0-2a2445842572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dd14beb-2436-424e-a5cd-40d08e857a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data(dataset = 'df_19_24_cleaned'):\n",
    "    return pd.read_pickle(f'../data/{dataset}.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "843f42fa-bc8d-4a7d-a1f7-a3b4dff2c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "def data_scaler(data):\n",
    "    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n",
    "    print('Data is scaled')\n",
    "    return data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59d842cc-21b1-48d4-be55-39e3eec77e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train-Test Split (keeping all hourly data points in the last 7 days of each month for testing)\n",
    "def train_test_split_7(data):\n",
    "    test_indices = data.index.to_series().groupby([data.index.year, data.index.month]).apply(lambda x: x[-24*7:])\n",
    "    test_data = data.loc[test_indices]\n",
    "    train_data = data.drop(test_indices)\n",
    "    print(f'Shape of train_data: {train_data.shape}')\n",
    "    print(f'Shape of test_data: {test_data.shape}')\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d31cba3-91ad-4c01-bcdf-e516f81ade5a",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4f4949f-c65e-494d-b0b2-a93761894052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length=24, target_column='price'):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data.iloc[i:i+seq_length].values)  # Include all features in X\n",
    "        y.append(data[target_column].iloc[i+seq_length])  # Target is still the original 'price'\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_sequences_2(data, seq_length=24, features = ['price']):\n",
    "    # features = ['price', 'wind_energy_generation', 'solar_energy_generation', 'total_load']\n",
    "    \n",
    "    # Convert to numpy array for easier slicing\n",
    "    data_array = data[features].values\n",
    "    \n",
    "    # Initialize lists for sequences and labels\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    # Create sequences\n",
    "    for i in range(len(data_array) - seq_length):\n",
    "        # Sequence of 24 time steps\n",
    "        sequences.append(data_array[i:i + seq_length])\n",
    "        \n",
    "        # The label is the price at the next time step after the sequence\n",
    "        labels.append(data_array[i + seq_length, 0])  # Assuming `price` is the first column\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    sequences = np.array(sequences)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return  sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f8c8ebc-d0f4-43ba-a155-d943698f9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=input_shape),\n",
    "        Dropout(0.2),  # Dropout to prevent overfitting\n",
    "        Dense(1)  # Output layer with a single neuron for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d8005-fd4c-4c7d-a2ab-a87954c4b094",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f17f36f-d9e7-4a5f-b6f3-3898eb2c8264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sMAPE function for evaluation\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef00cccf-eaa0-412e-9e45-a148692c0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_inverse(y_test_scaled, y_preds_scaled, X_test):\n",
    "    y_test_original = scaler.inverse_transform(\n",
    "        np.concatenate((y_test_scaled.reshape(-1, 1), X_test[:, -1, 1:]), axis=1))[:, 0]\n",
    "\n",
    "    y_preds_original = scaler.inverse_transform(\n",
    "        np.concatenate((y_preds_scaled, X_test[:, -1, 1:]), axis=1))[:, 0]\n",
    "\n",
    "    return y_test_original, y_preds_original\n",
    "    \n",
    "def scaler_inverse_2(y_test_scaled, y_preds_scaled, num_features = 1):\n",
    "    # Reshape predictions and true values for inverse transformation\n",
    "    y_preds_scaled = y_preds_scaled.reshape(-1, 1)\n",
    "    y_test_scaled = y_test_scaled.reshape(-1, 1)\n",
    "    \n",
    "    # Extend with zeros for other features to match scaler's input shape\n",
    "    # num_features = len(features)\n",
    "    zeros = np.zeros((len(y_preds_scaled), num_features - 1))\n",
    "    predictions_extended = np.concatenate([y_preds_scaled, zeros], axis=1)\n",
    "    # test\n",
    "    y_test_extended = np.concatenate([y_test, zeros], axis=1)\n",
    "    \n",
    "    # Inverse transform\n",
    "    y_preds_original = scaler.inverse_transform(predictions_extended)[:, 0]  # Only take price column\n",
    "    y_test_original = scaler.inverse_transform(y_test_extended)[:, 0]      \n",
    "\n",
    "    return y_test_original, y_preds_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba9a17d2-a6a7-4f0d-b227-334bd1916661",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def eva(y_test_scaled, y_preds_scaled, X_test):\n",
    "\n",
    "    # y_test, y_preds = scaler_inverse_2(y_test_scaled, y_preds_scaled)\n",
    "    y_test, y_preds = scaler_inverse(y_test_scaled, y_preds_scaled, X_test)\n",
    "    \n",
    "    lstm_smape = smape(y_test, y_preds)\n",
    "    print(f\"LSTM sMAPE: {lstm_smape:.2f}\")\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test, y_preds)\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_preds))\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "    return y_test, y_preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
