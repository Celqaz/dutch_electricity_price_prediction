{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef8b833-997d-4b67-b374-d1046343b4d3",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5a003ce-984b-4ff7-9ba0-2a2445842572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dd14beb-2436-424e-a5cd-40d08e857a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data(dataset = 'df_19_24_cleaned'):\n",
    "    data = pd.read_pickle(f'../data/{dataset}.pkl') \n",
    "    print(data.info())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "843f42fa-bc8d-4a7d-a1f7-a3b4dff2c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "def data_scaler(data):\n",
    "    data_scaled = pd.DataFrame(scaler.fit_transform(data), columns=data.columns, index=data.index)\n",
    "    print('Data is scaled')\n",
    "    return data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59d842cc-21b1-48d4-be55-39e3eec77e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Train-Test Split (keeping all hourly data points in the last 7 days of each month for testing)\n",
    "def train_test_split_7(data):\n",
    "    test_indices = data.index.to_series().groupby([data.index.year, data.index.month]).apply(lambda x: x[-24*7:])\n",
    "    test_data = data.loc[test_indices]\n",
    "    train_data = data.drop(test_indices)\n",
    "    print(f'Shape of train_data: {train_data.shape}')\n",
    "    print(f'Shape of test_data: {test_data.shape}')\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d31cba3-91ad-4c01-bcdf-e516f81ade5a",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4f4949f-c65e-494d-b0b2-a93761894052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length=24, target_column='price'):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data.iloc[i:i+seq_length].values)  # Include all features in X\n",
    "        y.append(data[target_column].iloc[i+seq_length])  # Target is still the original 'price'\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_sequences_2(data, seq_length=24, features = ['price']):\n",
    "    # features = ['price', 'wind_energy_generation', 'solar_energy_generation', 'total_load']\n",
    "    \n",
    "    # Convert to numpy array for easier slicing\n",
    "    data_array = data[features].values\n",
    "    \n",
    "    # Initialize lists for sequences and labels\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    # Create sequences\n",
    "    for i in range(len(data_array) - seq_length):\n",
    "        # Sequence of 24 time steps\n",
    "        sequences.append(data_array[i:i + seq_length])\n",
    "        \n",
    "        # The label is the price at the next time step after the sequence\n",
    "        labels.append(data_array[i + seq_length, 0])  # Assuming `price` is the first column\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    sequences = np.array(sequences)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return  sequences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f8c8ebc-d0f4-43ba-a155-d943698f9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=input_shape),\n",
    "        Dropout(0.2),  # Dropout to prevent overfitting\n",
    "        Dense(1)  # Output layer with a single neuron for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d8005-fd4c-4c7d-a2ab-a87954c4b094",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f17f36f-d9e7-4a5f-b6f3-3898eb2c8264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sMAPE function for evaluation\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef00cccf-eaa0-412e-9e45-a148692c0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler_inverse(y_test_scaled, y_preds_scaled, X_test):\n",
    "    y_test_original = scaler.inverse_transform(\n",
    "        np.concatenate((y_test_scaled.reshape(-1, 1), X_test[:, -1, 1:]), axis=1))[:, 0]\n",
    "\n",
    "    y_preds_original = scaler.inverse_transform(\n",
    "        np.concatenate((y_preds_scaled, X_test[:, -1, 1:]), axis=1))[:, 0]\n",
    "\n",
    "    return y_test_original, y_preds_original\n",
    "    \n",
    "def scaler_inverse_2(y_test_scaled, y_preds_scaled, num_features = 1):\n",
    "    # Reshape predictions and true values for inverse transformation\n",
    "    y_preds_scaled = y_preds_scaled.reshape(-1, 1)\n",
    "    y_test_scaled = y_test_scaled.reshape(-1, 1)\n",
    "    \n",
    "    # Extend with zeros for other features to match scaler's input shape\n",
    "    # num_features = len(features)\n",
    "    zeros = np.zeros((len(y_preds_scaled), num_features - 1))\n",
    "    predictions_extended = np.concatenate([y_preds_scaled, zeros], axis=1)\n",
    "    # test\n",
    "    y_test_extended = np.concatenate([y_test, zeros], axis=1)\n",
    "    \n",
    "    # Inverse transform\n",
    "    y_preds_original = scaler.inverse_transform(predictions_extended)[:, 0]  # Only take price column\n",
    "    y_test_original = scaler.inverse_transform(y_test_extended)[:, 0]      \n",
    "\n",
    "    return y_test_original, y_preds_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2682d0-f3c4-4ee1-a5bc-9ce2cd3f1569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "def eva(y_test, y_pred, X_test):\n",
    "    \n",
    "    # Inverse scale predictions and actual values\n",
    "    y_pred_rescaled = scaler.inverse_transform(\n",
    "        np.concatenate((y_pred, X_test[:, -1, 1:]), axis=1)\n",
    "    )[:, 0]\n",
    "    y_test_rescaled = scaler.inverse_transform(\n",
    "        np.concatenate((y_test.reshape(-1, 1), X_test[:, -1, 1:]), axis=1)\n",
    "    )[:, 0]\n",
    "    \n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "                   \n",
    "    def smape(y_true, y_pred):\n",
    "        return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "    \n",
    "    \n",
    "    smape_value = smape(y_test_rescaled, y_pred_rescaled)\n",
    "    print(f\"Symmetric Mean Absolute Percentage Error (sMAPE): {smape_value:.2f}\")\n",
    "    return y_test_rescaled, y_pred_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c54c06d5-7a9f-477c-ae30-e8e31f53dd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eva_s(y_test_rescaled, y_pred_rescaled):\n",
    "    \n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    \n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_rescaled, y_pred_rescaled))\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "                   \n",
    "    def smape(y_true, y_pred):\n",
    "        return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "    \n",
    "    \n",
    "    smape_value = smape(y_test_rescaled, y_pred_rescaled)\n",
    "    print(f\"Symmetric Mean Absolute Percentage Error (sMAPE): {smape_value:.2f}\")\n",
    "\n",
    "    return mae, rmse, smape_value\n",
    "    # return y_test_rescaled, y_pred_rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee6f31dd-77fa-482f-9627-05562472135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promo_prect(m_before, m_now, decrease = True ):\n",
    "    m1 = np.array(m_before)\n",
    "    m2 = np.array(m_now)\n",
    "    res = (m1 - m2) / m1 * 100\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2dd9c6-55d3-46ec-83a1-ac2aae17ed0b",
   "metadata": {},
   "source": [
    "# EWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9a1e194-b2c2-4811-965c-f9cf9103b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ewtpy\n",
    "\n",
    "# data: https://github.com/vrcarva/ewtpy/blob/master/ewtpy/ewtpy.py\n",
    "\n",
    "def ewt_decompose(data, K, log = 0, detect = \"locmax\", completion = 0, reg = 'average', lengthFilter = 10,sigmaFilter = 5):\n",
    "    ewt,  mfb ,boundaries = ewtpy.EWT1D(data, \n",
    "                                        N = K, \n",
    "                                        log = log, \n",
    "                                        detect = detect, \n",
    "                                        completion = completion, \n",
    "                                        reg = reg, \n",
    "                                        lengthFilter = lengthFilter,\n",
    "                                        sigmaFilter = sigmaFilter)\n",
    "    \n",
    "\n",
    "    combined_signal = np.sum(ewt, axis=1)\n",
    "    return combined_signal, ewt\n",
    "\n",
    "\n",
    "def plot_ewt(ewt, label = None ,start =None, end = None, ): \n",
    "    n = ewt.shape[1]\n",
    "    fig, axes = plt.subplots(n, 1, figsize=(12, 9))\n",
    "    for i in range(n):\n",
    "        axes[i].plot(ewt[start:end,i])\n",
    "        # axes[i].set_title(f'{name} EWT Component {i + 1}')\n",
    "    \n",
    "    # Set a shared ylabel for the entire plot\n",
    "    fig.text(-0.001, 0.5, label, va='center', rotation='vertical', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import ewtpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pywt import threshold\n",
    "\n",
    "# T = 1000\n",
    "# t = np.arange(1, T+1) / T\n",
    "# f = np.cos(2 * np.pi * 0.8 * t) + 2 * np.cos(2 * np.pi * 10 * t) + 0.8 * np.cos(2 * np.pi * 100 * t)\n",
    "\n",
    "def ewt_sureshrink_denoise(data, K, start=None, end=None, plot=False):\n",
    "    # Perform EWT decomposition\n",
    "    ewt, mfb, boundaries = ewtpy.EWT1D(data, N=K)\n",
    "    \n",
    "    # Apply SureShrink thresholding to each component\n",
    "    denoised_components = np.zeros_like(ewt)\n",
    "    for i in range(K):\n",
    "        component = ewt[:, i]\n",
    "        \n",
    "        # Calculate threshold based on the component's median absolute deviation (MAD)\n",
    "        sigma = np.median(np.abs(component - np.median(component))) / 0.6745\n",
    "        threshold_val = sigma * np.sqrt(2 * np.log(len(component)))\n",
    "        \n",
    "        # Apply soft thresholding to the component\n",
    "        denoised_components[:, i] = threshold(component, value=threshold_val, mode='soft')\n",
    "    \n",
    "    # Reconstruct the denoised signal by summing the denoised components\n",
    "    denoised_signal = np.sum(denoised_components, axis=1)\n",
    "    \n",
    "    # Plotting each component (optional)\n",
    "    if plot:\n",
    "        fig, axes = plt.subplots(K + 1, 1, figsize=(12, 9))\n",
    "        for i in range(K):\n",
    "            axes[i].plot(denoised_components[start:end, i], label=f'Denoised EWT Component {i + 1}')\n",
    "            axes[i].legend()\n",
    "        axes[-1].plot(denoised_signal[start:end], label='Reconstructed Signal', color='black')\n",
    "        axes[-1].legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return denoised_signal, denoised_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a828b7-3623-4dba-ab50-8af3537d2279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ewt_decompose(signal, K):\n",
    "#     # Perform Fourier Transform\n",
    "#     fourier_transform = fft(signal)\n",
    "#     components = []\n",
    "#     freq_range = np.linspace(0, np.pi, K + 1)\n",
    "    \n",
    "#     # Define filters and extract components\n",
    "#     for i in range(K):\n",
    "#         filter_mask = np.zeros_like(fourier_transform)\n",
    "#         left_boundary = freq_range[i]\n",
    "#         right_boundary = freq_range[i + 1]\n",
    "        \n",
    "#         for j in range(len(fourier_transform)):\n",
    "#             frequency = j * np.pi / len(fourier_transform)\n",
    "#             if left_boundary <= frequency <= right_boundary:\n",
    "#                 filter_mask[j] = 1\n",
    "        \n",
    "#         # Apply filter and inverse FFT\n",
    "#         component_fft = fourier_transform * filter_mask\n",
    "#         component_ifft = ifft(component_fft).real  # Convert back to time domain\n",
    "#         components.append(component_ifft)\n",
    "    \n",
    "#     return np.array(components).T  # Shape: (t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332f1c6b-e466-49bc-b24e-759e45f5e9fd",
   "metadata": {},
   "source": [
    "## Stat Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aeed9d0-fd70-4f40-9d7f-311891ec2b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85f8527-a8c6-4cf7-8bc5-2b1db879a4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(data):\n",
    "    adf_test = adfuller(data, regression='c')\n",
    "    print('ADF Statistic: {:.6f}\\np-value: {:.6f}\\n#Lags used: {}'\n",
    "          .format(adf_test[0], adf_test[1], adf_test[2]))\n",
    "    for key, value in adf_test[4].items():\n",
    "        print('Critical Value ({}): {:.6f}'.format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e38e054-2248-4182-8ab1-18463a3ae294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dm_test_cus(actual_lst, pred1_lst, pred2_lst, h = 1, crit=\"MSE\", power = 2):\n",
    "    # Routine for checking errors\n",
    "    def error_check():\n",
    "        rt = 0\n",
    "        msg = \"\"\n",
    "        # Check if h is an integer\n",
    "        if (not isinstance(h, int)):\n",
    "            rt = -1\n",
    "            msg = \"The type of the number of steps ahead (h) is not an integer.\"\n",
    "            return (rt,msg)\n",
    "        # Check the range of h\n",
    "        if (h < 1):\n",
    "            rt = -1\n",
    "            msg = \"The number of steps ahead (h) is not large enough.\"\n",
    "            return (rt,msg)\n",
    "        len_act = len(actual_lst)\n",
    "        len_p1  = len(pred1_lst)\n",
    "        len_p2  = len(pred2_lst)\n",
    "        # Check if lengths of actual values and predicted values are equal\n",
    "        if (len_act != len_p1 or len_p1 != len_p2 or len_act != len_p2):\n",
    "            rt = -1\n",
    "            msg = \"Lengths of actual_lst, pred1_lst and pred2_lst do not match.\"\n",
    "            return (rt,msg)\n",
    "        # Check range of h\n",
    "        if (h >= len_act):\n",
    "            rt = -1\n",
    "            msg = \"The number of steps ahead is too large.\"\n",
    "            return (rt,msg)\n",
    "        # Check if criterion supported\n",
    "        if (crit != \"MSE\" and crit != \"MAPE\" and crit != \"MAD\" and crit != \"poly\"):\n",
    "            rt = -1\n",
    "            msg = \"The criterion is not supported.\"\n",
    "            return (rt,msg)  \n",
    "        # Check if every value of the input lists are numerical values\n",
    "        from re import compile as re_compile\n",
    "        comp = re_compile(\"^\\d+?\\.\\d+?$\")  \n",
    "        def compiled_regex(s):\n",
    "            \"\"\" Returns True is string is a number. \"\"\"\n",
    "            if comp.match(s) is None:\n",
    "                return s.isdigit()\n",
    "            return True\n",
    "        for actual, pred1, pred2 in zip(actual_lst, pred1_lst, pred2_lst):\n",
    "            is_actual_ok = compiled_regex(str(abs(actual)))\n",
    "            is_pred1_ok = compiled_regex(str(abs(pred1)))\n",
    "            is_pred2_ok = compiled_regex(str(abs(pred2)))\n",
    "            if (not (is_actual_ok and is_pred1_ok and is_pred2_ok)):  \n",
    "                msg = \"An element in the actual_lst, pred1_lst or pred2_lst is not numeric.\"\n",
    "                rt = -1\n",
    "                return (rt,msg)\n",
    "        return (rt,msg)\n",
    "    \n",
    "    # Error check\n",
    "    error_code = error_check()\n",
    "    # Raise error if cannot pass error check\n",
    "    if (error_code[0] == -1):\n",
    "        raise SyntaxError(error_code[1])\n",
    "        return\n",
    "    # Import libraries\n",
    "    from scipy.stats import t\n",
    "    import collections\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Initialise lists\n",
    "    e1_lst = []\n",
    "    e2_lst = []\n",
    "    d_lst  = []\n",
    "    \n",
    "    # convert every value of the lists into real values\n",
    "    actual_lst = pd.Series(actual_lst).apply(lambda x: float(x)).tolist()\n",
    "    pred1_lst = pd.Series(pred1_lst).apply(lambda x: float(x)).tolist()\n",
    "    pred2_lst = pd.Series(pred2_lst).apply(lambda x: float(x)).tolist()\n",
    "    \n",
    "    # Length of lists (as real numbers)\n",
    "    T = float(len(actual_lst))\n",
    "    \n",
    "    # construct d according to crit\n",
    "    if (crit == \"MSE\"):\n",
    "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
    "            e1_lst.append((actual - p1)**2)\n",
    "            e2_lst.append((actual - p2)**2)\n",
    "        for e1, e2 in zip(e1_lst, e2_lst):\n",
    "            d_lst.append(e1 - e2)\n",
    "    elif (crit == \"MAD\"):\n",
    "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
    "            e1_lst.append(abs(actual - p1))\n",
    "            e2_lst.append(abs(actual - p2))\n",
    "        for e1, e2 in zip(e1_lst, e2_lst):\n",
    "            d_lst.append(e1 - e2)\n",
    "    elif (crit == \"MAPE\"):\n",
    "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
    "            e1_lst.append(abs((actual - p1)/actual))\n",
    "            e2_lst.append(abs((actual - p2)/actual))\n",
    "        for e1, e2 in zip(e1_lst, e2_lst):\n",
    "            d_lst.append(e1 - e2)\n",
    "    elif (crit == \"poly\"):\n",
    "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
    "            e1_lst.append(((actual - p1))**(power))\n",
    "            e2_lst.append(((actual - p2))**(power))\n",
    "        for e1, e2 in zip(e1_lst, e2_lst):\n",
    "            d_lst.append(e1 - e2)    \n",
    "    \n",
    "    # Mean of d        \n",
    "    mean_d = pd.Series(d_lst).mean()\n",
    "    \n",
    "    # Find autocovariance and construct DM test statistics\n",
    "    def autocovariance(Xi, N, k, Xs):\n",
    "        autoCov = 0\n",
    "        T = float(N)\n",
    "        for i in np.arange(0, N-k):\n",
    "              autoCov += ((Xi[i+k])-Xs)*(Xi[i]-Xs)\n",
    "        return (1/(T))*autoCov\n",
    "    gamma = []\n",
    "    for lag in range(0,h):\n",
    "        gamma.append(autocovariance(d_lst,len(d_lst),lag,mean_d)) # 0, 1, 2\n",
    "    V_d = (gamma[0] + 2*sum(gamma[1:]))/T\n",
    "    DM_stat=V_d**(-0.5)*mean_d\n",
    "    harvey_adj=((T+1-2*h+h*(h-1)/T)/T)**(0.5)\n",
    "    DM_stat = harvey_adj*DM_stat\n",
    "    # Find p-value\n",
    "    p_value = 2*t.cdf(-abs(DM_stat), df = T - 1)\n",
    "    \n",
    "    # Construct named tuple for return\n",
    "    dm_return = collections.namedtuple('dm_return', 'DM p_value')\n",
    "    \n",
    "    rt = dm_return(DM = DM_stat, p_value = p_value)\n",
    "    \n",
    "    return rt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b968e651-e0b3-4980-8390-c3cd0919c3eb",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2605ceb3-10da-4572-88ec-8c4852613730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "def plot_acf_pacf(data):\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(10, 6))\n",
    "    plot_acf(data, lags=50, ax=ax1)\n",
    "    plot_pacf(data, lags=50, ax=ax2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc04090c-83a5-426e-ab7a-3f458756d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_preds(test, preds):\n",
    "    fig, ax = plt.subplots(figsize = (12,6))\n",
    "    # ax.plot(train['date'], train['data'], 'g-.', label='Train')\n",
    "    ax.plot(test, 'b-', label='Test')\n",
    "    ax.plot(preds, 'r--', label='Predicted')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Electricity Price')\n",
    "    # ax.axvspan(80, 83, color='#808080', alpha=0.2)\n",
    "    ax.legend(loc=2)\n",
    "    \n",
    "    \n",
    "    fig.autofmt_xdate()\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0ffe26-d451-4d5c-95bf-b3e465417d8d",
   "metadata": {},
   "source": [
    "## Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39e9b786-a4a0-4bb3-8d9a-a3fff2e99f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedScaler:\n",
    "    def __init__(self):\n",
    "        self.median = None\n",
    "        self.sum_abs_dev = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        # Calculate the median and sum of absolute deviations from the median\n",
    "        data = np.array(data)\n",
    "        self.median = np.median(data)\n",
    "        self.sum_abs_dev = np.sum(np.abs(data - self.median))\n",
    "\n",
    "    def transform(self, data):\n",
    "        # Apply the custom normalization formula\n",
    "        data = np.array(data)\n",
    "        return (data - self.median) / self.sum_abs_dev\n",
    "\n",
    "    def inverse_transform(self, scaled_data):\n",
    "        # Reverse the transformation to get the original data\n",
    "        scaled_data = np.array(scaled_data)\n",
    "        return scaled_data * self.sum_abs_dev + self.median\n",
    "\n",
    "# # Example usage\n",
    "# data = [1, 2, 3, 4, 5]  # Replace with your actual data\n",
    "# scaler = CustomScaler()\n",
    "# scaler.fit(data)  # Fit to the original data\n",
    "\n",
    "# # Transform the data\n",
    "# scaled_data = scaler.transform(data)\n",
    "# print(\"Scaled Data:\", scaled_data)\n",
    "\n",
    "# # Inverse transform to recover the original data\n",
    "# original_data = scaler.inverse_transform(scaled_data)\n",
    "# print(\"Recovered Original Data:\", original_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8cd04-8133-4b37-b159-e668ff321f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
